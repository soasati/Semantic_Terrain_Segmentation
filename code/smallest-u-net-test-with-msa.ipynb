{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\"\"\"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"0dd16fdb-55b7-4755-8593-a9aab0787a32","_cell_guid":"b5595bf5-1876-473b-92cf-554357e900e7","collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:14:58.511116Z","iopub.execute_input":"2023-05-14T19:14:58.511489Z","iopub.status.idle":"2023-05-14T19:14:58.544930Z","shell.execute_reply.started":"2023-05-14T19:14:58.511408Z","shell.execute_reply":"2023-05-14T19:14:58.543934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set up configuration","metadata":{"_uuid":"e9096087-60e0-4f0e-8663-a1c3ad22f971","_cell_guid":"49ce4cd9-a7fa-4a19-925e-9ed984977078","trusted":true}},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"","metadata":{"_uuid":"9c3b519d-cd87-48ad-9469-1b02fb4ddbd6","_cell_guid":"a0b6ee56-f306-4e50-9d52-be0a39fee4cb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:14:58.548910Z","iopub.execute_input":"2023-05-14T19:14:58.551082Z","iopub.status.idle":"2023-05-14T19:14:58.555469Z","shell.execute_reply.started":"2023-05-14T19:14:58.551051Z","shell.execute_reply":"2023-05-14T19:14:58.554214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install segmentation-models-pytorch\n!pip install -U git+https://github.com/albumentations-team/albumentations\n!pip install --upgrade opencv-contrib-python","metadata":{"_uuid":"be015055-f85e-4762-94f4-38f1077797c9","_cell_guid":"553421c7-55e8-452a-8905-4677df122ebd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:14:58.566478Z","iopub.execute_input":"2023-05-14T19:14:58.566754Z","iopub.status.idle":"2023-05-14T19:15:30.246026Z","shell.execute_reply.started":"2023-05-14T19:14:58.566728Z","shell.execute_reply":"2023-05-14T19:15:30.244605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install torch==1.11.0","metadata":{"_uuid":"109daa86-3f36-4dd4-90c0-83660d9119dc","_cell_guid":"d86c0c85-f745-4f1f-92d3-ee764d1d3637","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:30.249069Z","iopub.execute_input":"2023-05-14T19:15:30.250247Z","iopub.status.idle":"2023-05-14T19:15:30.255231Z","shell.execute_reply.started":"2023-05-14T19:15:30.250201Z","shell.execute_reply":"2023-05-14T19:15:30.254211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport cv2\nimport torch\nimport torchvision\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \n\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport csv","metadata":{"_uuid":"079ab000-c55b-4e15-a2cf-e8a615dd3b43","_cell_guid":"d283d225-5a85-4ea2-8fd1-ed882c8daca8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:30.256861Z","iopub.execute_input":"2023-05-14T19:15:30.257304Z","iopub.status.idle":"2023-05-14T19:15:32.729801Z","shell.execute_reply.started":"2023-05-14T19:15:30.257244Z","shell.execute_reply":"2023-05-14T19:15:32.728770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.__version__)","metadata":{"_uuid":"a34a4698-9e84-46da-85d8-a2c3d4ab8351","_cell_guid":"84bbf49f-4f61-4054-9afc-6244e93ce0b0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:32.732573Z","iopub.execute_input":"2023-05-14T19:15:32.733098Z","iopub.status.idle":"2023-05-14T19:15:32.738647Z","shell.execute_reply.started":"2023-05-14T19:15:32.733066Z","shell.execute_reply":"2023-05-14T19:15:32.737261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set-up configurations","metadata":{"_uuid":"1637569c-f932-4f64-a57d-e2fb799a1454","_cell_guid":"ab4dca38-57dc-4741-9e0e-30f0a1461d26","trusted":true}},{"cell_type":"code","source":"IMAGE_PATH=\"/kaggle/input/vale-semantic-terrain-segmentation/raw_images/raw_images/\"\nMASK_PATH=\"/kaggle/input/vale-semantic-terrain-segmentation/mask_rgb_filled/mask_rgb_filled/\"\nDEVICE=torch.device('cuda')\nEPOCHS=100\nLR=0.0000025\nBATCH_SIZE=15\nLOAD_MODEL=False","metadata":{"_uuid":"39c7e8a4-0de5-4d99-a3ca-202eb0aaec49","_cell_guid":"f8d83965-23a4-43da-9b3f-7c6dc15c4d93","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:32.740592Z","iopub.execute_input":"2023-05-14T19:15:32.741328Z","iopub.status.idle":"2023-05-14T19:15:32.751496Z","shell.execute_reply.started":"2023-05-14T19:15:32.741287Z","shell.execute_reply":"2023-05-14T19:15:32.750603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bad images","metadata":{"_uuid":"13040ea9-5fc6-45f5-b479-89985e0b9580","_cell_guid":"d89e401f-c45a-4563-8769-92c2a9f1f648","trusted":true}},{"cell_type":"code","source":"bad_images=[]\n\"\"\"\nfor test_image in os.listdir(IMAGE_PATH):\n    try:\n        image=cv2.imread(IMAGE_PATH+test_image)\n        image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    except Exception as err:\n        print(err)\n        bad_images.append(test_image)\n\"\"\"\nbad_images.append(\"05200.png\")\nprint(bad_images)","metadata":{"_uuid":"ccf95db1-db66-4602-bcf9-24b81e15d965","_cell_guid":"24984a4e-710c-4712-8c4b-9c30a7008666","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:32.753130Z","iopub.execute_input":"2023-05-14T19:15:32.753513Z","iopub.status.idle":"2023-05-14T19:15:32.763525Z","shell.execute_reply.started":"2023-05-14T19:15:32.753476Z","shell.execute_reply":"2023-05-14T19:15:32.762494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert segmentation masks into one-hot-encoding.","metadata":{"_uuid":"87229f66-57f0-490c-818c-6df4d62d0ede","_cell_guid":"521907ae-5c3a-4489-b09f-34407ad5e9ea","trusted":true}},{"cell_type":"code","source":"def get_one_hot_encoding(mask):\n    #Yellow matrix\n    YELLOW = np.array([255, 255, 0])\n    YELLOW_Matrix = np.zeros((1080, 1920))\n    yellow_pixels = np.all(mask == YELLOW, axis=2)\n    YELLOW_Matrix[yellow_pixels] = 1\n\n    #Red matrix\n    RED=np.array([255,0,0])\n    RED_Matrix=np.zeros((1080,1920))\n    red_pixels=np.all(mask== RED,axis=2)\n    RED_Matrix[red_pixels]=1\n\n    #Green matrix\n    GREEN=np.array([0,255,0])\n    GREEN_Matrix=np.zeros((1080,1920))\n    green_pixels=np.all(mask== GREEN,axis=2)\n    GREEN_Matrix[green_pixels]=1\n\n    #Orange matrix\n    ORANGE=np.array([255,128,0])\n    ORANGE_Matrix=np.zeros((1080,1920))\n    orange_pixels=np.all(mask== ORANGE,axis=2)\n    ORANGE_Matrix[orange_pixels]=1\n\n    stacked=np.stack((YELLOW_Matrix,RED_Matrix,GREEN_Matrix,ORANGE_Matrix),axis=2)\n    return stacked","metadata":{"_uuid":"de86ee03-28be-481d-ba7d-d9fbc472edfd","_cell_guid":"07c852a5-9ffa-41bf-95c0-033e1e07096f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:32.765016Z","iopub.execute_input":"2023-05-14T19:15:32.765702Z","iopub.status.idle":"2023-05-14T19:15:32.779067Z","shell.execute_reply.started":"2023-05-14T19:15:32.765665Z","shell.execute_reply":"2023-05-14T19:15:32.777786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot_decoding(one_hot_encoded):\n    # Prepare the blank image with the same dimensions as the input\n    decoded_image = np.zeros((one_hot_encoded.shape[0], one_hot_encoded.shape[1], 3), dtype=np.uint8)\n\n    # Yellow\n    YELLOW = np.array([255, 255, 0])\n    yellow_pixels = one_hot_encoded[:, :, 0] == 1\n    decoded_image[yellow_pixels] = YELLOW\n\n    # Red\n    RED = np.array([255, 0, 0])\n    red_pixels = one_hot_encoded[:, :, 1] == 1\n    decoded_image[red_pixels] = RED\n\n    # Green\n    GREEN = np.array([0, 255, 0])\n    green_pixels = one_hot_encoded[:, :, 2] == 1\n    decoded_image[green_pixels] = GREEN\n\n    # Orange\n    ORANGE = np.array([255, 128, 0])\n    orange_pixels = one_hot_encoded[:, :, 3] == 1\n    decoded_image[orange_pixels] = ORANGE\n\n    return decoded_image","metadata":{"_uuid":"9b1e625c-f69f-4c99-a71f-410350d0c1cd","_cell_guid":"ac7d3490-0778-4421-8c92-dfa67b3c0f3a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:32.780715Z","iopub.execute_input":"2023-05-14T19:15:32.781672Z","iopub.status.idle":"2023-05-14T19:15:32.791385Z","shell.execute_reply.started":"2023-05-14T19:15:32.781636Z","shell.execute_reply":"2023-05-14T19:15:32.790375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split into sixteen and reconstruct","metadata":{"_uuid":"fd5f0423-837b-4327-b454-04ce0b7b76c0","_cell_guid":"d490ebb5-65e8-44ab-9f50-9311267a23ec","trusted":true}},{"cell_type":"code","source":"def split_into_sixteen(image,mask, rows=4, cols=4):\n    # Get the dimensions of the input image\n    if(len(image.shape)==3): #Loaded using cv2\n        height, width, _ = image.shape\n    else: #Loaded as tensor\n        n,c,height,width=image.shape\n\n    # Calculate the size of each block\n    block_height = height // rows\n    block_width = width // cols\n\n    # Initialize an empty list to store the divided image parts\n    divided_image_parts = []\n    divided_mask_parts=[]\n    # Loop through the image grid and append each part to the divided_parts list\n    #print(\"type(Image)= \",type(image))\n    for i in range(rows):\n        for j in range(cols):\n            # Compute the coordinates of the current block\n            y_start = i * block_height\n            y_end = (i + 1) * block_height\n            x_start = j * block_width\n            x_end = (j + 1) * block_width\n\n            # Crop the image and append it to the divided_parts list\n            if(isinstance(image,torch.Tensor)):\n                #print(\"Tensor\")\n                divided_image_parts.append(image[:,:,y_start:y_end,x_start:x_end])\n                divided_mask_parts.append(mask[:,:,y_start:y_end,x_start:x_end])\n            \n            else:\n                divided_image_parts.append(image[y_start:y_end, x_start:x_end,:])\n                divided_mask_parts.append(mask[y_start:y_end,x_start:x_end,:])\n\n    return divided_image_parts,divided_mask_parts\n\ndef reconstruct_image(divided_image_parts, divided_mask_parts,rows=4, cols=4):\n    image_rows = []\n    mask_rows=[]\n\n    for i in range(rows):\n        image_row = np.hstack(divided_image_parts[i * cols : (i + 1) * cols])\n        mask_row=np.hstack(divided_mask_parts[i*cols: (i+1)*cols])\n        image_rows.append(image_row)\n        mask_rows.append(mask_row)\n\n    reconstructed_image = np.vstack(image_rows)\n    reconstructed_mask=np.vstack(mask_rows)\n    \n    return reconstructed_image,reconstructed_mask","metadata":{"_uuid":"820037bd-44be-4237-8a1f-d609f88e7477","_cell_guid":"a350f61a-6439-4368-851f-8f3ae879db18","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:32.792847Z","iopub.execute_input":"2023-05-14T19:15:32.793536Z","iopub.status.idle":"2023-05-14T19:15:32.808157Z","shell.execute_reply.started":"2023-05-14T19:15:32.793497Z","shell.execute_reply":"2023-05-14T19:15:32.807490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentations","metadata":{"_uuid":"e8f26a1f-da12-434b-af59-6f3fe2bb9bcd","_cell_guid":"43468516-e2f9-414d-86c3-08a07ca9ab77","trusted":true}},{"cell_type":"code","source":"import albumentations as A","metadata":{"_uuid":"f58f1c42-c2ba-4057-b066-8b8f08b393f0","_cell_guid":"6f6edbc5-11be-44fa-af59-e2ede03a9c38","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:32.812940Z","iopub.execute_input":"2023-05-14T19:15:32.813212Z","iopub.status.idle":"2023-05-14T19:15:33.805221Z","shell.execute_reply.started":"2023-05-14T19:15:32.813186Z","shell.execute_reply":"2023-05-14T19:15:33.804231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_augs():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Rotate(limit=30, p=0.5),\n        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n        A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n        A.CoarseDropout(max_holes=8, max_height=30, max_width=30, min_holes=2, min_height=10, min_width=10, p=0.3),\n        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.5),\n    ])","metadata":{"_uuid":"b9e67dcb-fac2-4dc4-b94e-5fbb342fa415","_cell_guid":"78b9bc59-6ae5-481b-bb6c-4dd3635f745f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:33.806842Z","iopub.execute_input":"2023-05-14T19:15:33.807220Z","iopub.status.idle":"2023-05-14T19:15:33.816253Z","shell.execute_reply.started":"2023-05-14T19:15:33.807183Z","shell.execute_reply":"2023-05-14T19:15:33.815310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CSV FILE AND Pandas Dataframe","metadata":{"_uuid":"cd7ef027-28e9-4015-a632-9a299a1722c6","_cell_guid":"96080030-f5b7-43a0-a2db-20054e2ae176","trusted":true}},{"cell_type":"code","source":"CSV_FILENAME=\"file.csv\"\nwith open(CSV_FILENAME,'w') as csvfile:\n    fwriter=csv.writer(csvfile,delimiter=',',\n                      quotechar=',', quoting=csv.QUOTE_MINIMAL)\n    fwriter.writerow(['image_path','mask_path'])\n    for name in os.listdir(IMAGE_PATH):\n        mask=cv2.imread(MASK_PATH+name)\n        mask=cv2.cvtColor(mask,cv2.COLOR_BGR2RGB)\n        fwriter.writerow([IMAGE_PATH+name,MASK_PATH+name])\n\n\ndf=pd.read_csv(CSV_FILENAME)\ndf.head()\nprint(len(df))","metadata":{"_uuid":"fdcf04a2-b15e-43ff-8096-704a32e5bcf7","_cell_guid":"1244472b-c694-4f2a-9c15-b00d067576b0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:33.817640Z","iopub.execute_input":"2023-05-14T19:15:33.818216Z","iopub.status.idle":"2023-05-14T19:15:50.820582Z","shell.execute_reply.started":"2023-05-14T19:15:33.818178Z","shell.execute_reply":"2023-05-14T19:15:50.819531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Drop bad images\nfor bad_image in bad_images:\n    df=df.drop(df[df['image_path']==IMAGE_PATH+bad_image].index)\nprint(len(df))","metadata":{"_uuid":"92414282-90a5-4c7b-a98d-084f7c4d6c53","_cell_guid":"e598dd9e-816b-4f92-8223-8bb911267f3c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:50.822056Z","iopub.execute_input":"2023-05-14T19:15:50.823534Z","iopub.status.idle":"2023-05-14T19:15:50.840773Z","shell.execute_reply.started":"2023-05-14T19:15:50.823493Z","shell.execute_reply":"2023-05-14T19:15:50.839506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train-val-test split","metadata":{"_uuid":"d1e1348e-0f9a-4b03-958a-2beefbe5c656","_cell_guid":"8e839911-b106-4f25-befc-3d0ca7b3b5a3","trusted":true}},{"cell_type":"code","source":"df_train,df_test=train_test_split(df,test_size=0.2,random_state=42)\ndf_val,df_test=train_test_split(df_test,test_size=0.5,random_state=42)\nprint(\"size of df_train= \",len(df_train))\nprint(\"size of df_val= \",len(df_val))\nprint(\"size of df_test= \", len(df_test))","metadata":{"_uuid":"37c54d10-5f25-4008-9497-b876cc92bfb4","_cell_guid":"94f8188a-5e7d-488d-8fd7-f2aa528adeb8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:50.842236Z","iopub.execute_input":"2023-05-14T19:15:50.842736Z","iopub.status.idle":"2023-05-14T19:15:50.858185Z","shell.execute_reply.started":"2023-05-14T19:15:50.842695Z","shell.execute_reply":"2023-05-14T19:15:50.857025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"_uuid":"8a4afa51-4941-4213-ae17-42d8e80628c7","_cell_guid":"2d9a41ec-be78-47e7-b6d6-3ebb85be8615","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:50.859706Z","iopub.execute_input":"2023-05-14T19:15:50.860180Z","iopub.status.idle":"2023-05-14T19:15:50.878882Z","shell.execute_reply.started":"2023-05-14T19:15:50.860144Z","shell.execute_reply":"2023-05-14T19:15:50.877842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val.head()","metadata":{"_uuid":"5cc04514-042d-45de-b27e-8f06313df483","_cell_guid":"7df4514a-389d-4d6f-a090-2f626a19e530","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:50.880367Z","iopub.execute_input":"2023-05-14T19:15:50.880924Z","iopub.status.idle":"2023-05-14T19:15:50.892365Z","shell.execute_reply.started":"2023-05-14T19:15:50.880888Z","shell.execute_reply":"2023-05-14T19:15:50.891366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"_uuid":"fb0ddd1a-1633-47d0-82c0-008462bb2adf","_cell_guid":"d8f06583-3fc7-4109-9c0c-408f7c5bb0f6","collapsed":false,"execution":{"iopub.status.busy":"2023-05-14T19:15:50.893982Z","iopub.execute_input":"2023-05-14T19:15:50.894358Z","iopub.status.idle":"2023-05-14T19:15:50.909323Z","shell.execute_reply.started":"2023-05-14T19:15:50.894319Z","shell.execute_reply":"2023-05-14T19:15:50.908379Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image=df_test.loc[24,'image_path']\nprint(image)\nmask=df_test.loc[24,'mask_path']\nprint(mask)","metadata":{"_uuid":"1b0ee046-4421-4650-9ca6-91989d9e5a1c","_cell_guid":"7df2682c-172f-4bca-86d3-bacef8264cec","collapsed":false,"execution":{"iopub.status.busy":"2023-05-14T19:15:50.911200Z","iopub.execute_input":"2023-05-14T19:15:50.911928Z","iopub.status.idle":"2023-05-14T19:15:50.922119Z","shell.execute_reply.started":"2023-05-14T19:15:50.911891Z","shell.execute_reply":"2023-05-14T19:15:50.920944Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image","metadata":{"_uuid":"12ab8afd-47c5-4996-902c-eede02083ad7","_cell_guid":"bd8a3835-87c8-48af-961e-59f9d2420c5e","collapsed":false,"execution":{"iopub.status.busy":"2023-05-14T19:15:50.923601Z","iopub.execute_input":"2023-05-14T19:15:50.923951Z","iopub.status.idle":"2023-05-14T19:15:50.934178Z","shell.execute_reply.started":"2023-05-14T19:15:50.923917Z","shell.execute_reply":"2023-05-14T19:15:50.933146Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the images\nimage_path = \"/kaggle/input/vale-semantic-terrain-segmentation/raw_images/raw_images/05077.png\"\nmask_path = \"/kaggle/input/vale-semantic-terrain-segmentation/mask_rgb_filled/mask_rgb_filled/05077.png\"\n\n# Open and display the images\nimage = Image.open(image_path)\nmask = Image.open(mask_path)\n\n# Create a figure with two subplots\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plot the image on the left subplot\naxs[0].imshow(image)\naxs[0].set_title(\"Image\")\n\n# Plot the mask on the right subplot\naxs[1].imshow(mask)\naxs[1].set_title(\"Mask\")\n\n# Remove the axis labels\nfor ax in axs:\n    ax.axis(\"off\")\n\n# Adjust the spacing between subplots\nplt.tight_layout()\n\n# Display the figure\nplt.show()","metadata":{"_uuid":"4431ecfc-d207-4c60-b180-7b95391eecb4","_cell_guid":"c69f5e59-e950-4344-85d6-5be6b3848054","collapsed":false,"execution":{"iopub.status.busy":"2023-05-14T19:15:50.935887Z","iopub.execute_input":"2023-05-14T19:15:50.936348Z","iopub.status.idle":"2023-05-14T19:15:51.953284Z","shell.execute_reply.started":"2023-05-14T19:15:50.936313Z","shell.execute_reply":"2023-05-14T19:15:51.952320Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Dataset","metadata":{"_uuid":"a4a743c6-353a-4e52-8676-498e566fafcc","_cell_guid":"1b3a1026-3de3-4941-a803-d420159afe0f","trusted":true}},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchvision import transforms","metadata":{"_uuid":"073b41e1-5dd9-4a9c-bf91-f00445eeb88c","_cell_guid":"1677d28d-0e93-4fa4-a31b-d03262f77b14","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:51.954979Z","iopub.execute_input":"2023-05-14T19:15:51.955699Z","iopub.status.idle":"2023-05-14T19:15:51.964857Z","shell.execute_reply.started":"2023-05-14T19:15:51.955660Z","shell.execute_reply":"2023-05-14T19:15:51.963834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AugmentationDataset(Dataset):\n  def __init__(self,df,augmentation):\n    self.df=df\n    self.augmentations=augmentation\n    self.to_tensor=transforms.ToTensor()\n  def __len__(self):\n    return len(self.df)\n  \n  def __getitem__(self,idx):\n    row=self.df.iloc[idx]\n    image_path=row.image_path\n    mask_path=row.mask_path\n    #print(\"Image path= \",image_path)\n    #print(\"Mask path= \",mask_path)\n    #Load the image and mask\n    image=cv2.imread(image_path)\n    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    mask=cv2.imread(mask_path)\n    mask=cv2.cvtColor(mask,cv2.COLOR_BGR2RGB)\n    \n    #Apply the augmentations\n    if self.augmentations:\n      data=self.augmentations(image=image,mask=mask)\n      image=data['image']\n      mask=data['mask']\n    \n    #Get the one-hot-encoding\n    encoded_mask=get_one_hot_encoding(mask)\n    #Convert the image and mask to tensor\n    image=self.to_tensor(image)\n    encoded_mask=self.to_tensor(encoded_mask)\n    \n    return image,encoded_mask","metadata":{"_uuid":"74210d1c-7d28-4cf4-bc59-322228cf1849","_cell_guid":"08f343c0-af04-419e-acab-0eb161a5622f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:51.966544Z","iopub.execute_input":"2023-05-14T19:15:51.967203Z","iopub.status.idle":"2023-05-14T19:15:51.986455Z","shell.execute_reply.started":"2023-05-14T19:15:51.967168Z","shell.execute_reply":"2023-05-14T19:15:51.985605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set=AugmentationDataset(df_train,get_augs())\nval_set=AugmentationDataset(df_val,get_augs())\ntrain_val_set=torch.utils.data.ConcatDataset([train_set,val_set])\ntest_set=AugmentationDataset(df_test,get_augs())","metadata":{"_uuid":"223359bc-226c-44c8-9841-c27e50d07a1a","_cell_guid":"dadbf6a7-8f17-4de0-a8af-94283b36ef9a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:51.988237Z","iopub.execute_input":"2023-05-14T19:15:51.988923Z","iopub.status.idle":"2023-05-14T19:15:52.013348Z","shell.execute_reply.started":"2023-05-14T19:15:51.988878Z","shell.execute_reply":"2023-05-14T19:15:52.012394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"0272b5ee-5458-4378-9eca-a78fddbbe4b9","_cell_guid":"73762018-0782-4e8b-9b1f-40cd39635e94","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Train set size={len(train_set)}\")\nprint(f\"Val set size={len(val_set)}\")\nprint(f\"Train val set size= {len(train_val_set)}\")\nprint(f\"Test set size= {len(test_set)}\")","metadata":{"_uuid":"3e83ceef-c5e4-4688-8794-9dc908db8b56","_cell_guid":"e70bd6d4-8e54-4460-a2c1-cc1fcc954559","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:52.014998Z","iopub.execute_input":"2023-05-14T19:15:52.015720Z","iopub.status.idle":"2023-05-14T19:15:52.031220Z","shell.execute_reply.started":"2023-05-14T19:15:52.015684Z","shell.execute_reply":"2023-05-14T19:15:52.030238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Dataset Loader","metadata":{"_uuid":"fbfa6c81-b6be-4362-868c-95f36c659192","_cell_guid":"762c2c42-3fdf-4cba-b406-7c7620a2ab80","trusted":true}},{"cell_type":"code","source":"from torch.utils.data import DataLoader","metadata":{"_uuid":"1e913e34-323e-43a8-ab2f-c230cc68b2af","_cell_guid":"4f0a7eea-d8b2-45da-aa11-de87d11575d3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:52.033061Z","iopub.execute_input":"2023-05-14T19:15:52.033758Z","iopub.status.idle":"2023-05-14T19:15:52.044397Z","shell.execute_reply.started":"2023-05-14T19:15:52.033722Z","shell.execute_reply":"2023-05-14T19:15:52.043389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainLoader=DataLoader(train_set,batch_size=BATCH_SIZE,shuffle=True)\nvalLoader=DataLoader(val_set,batch_size=BATCH_SIZE,shuffle=True)\ntrainValLoader=DataLoader(train_val_set,batch_size=BATCH_SIZE,shuffle=True)\ntestLoader=DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=True)","metadata":{"_uuid":"dcc03baa-5ce3-4718-9377-a6540a4a542a","_cell_guid":"fccc2669-0e3d-4fce-afc4-3280e76cd832","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:52.045944Z","iopub.execute_input":"2023-05-14T19:15:52.046686Z","iopub.status.idle":"2023-05-14T19:15:52.056161Z","shell.execute_reply.started":"2023-05-14T19:15:52.046651Z","shell.execute_reply":"2023-05-14T19:15:52.055277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"# of batches in trainLoader={len(trainLoader)}\")\nprint(f\"# of batches in valLoader={len(valLoader)}\")\nprint(f\"# of batches in train val loader={len(valLoader)}\")\nprint(f\"# of batches in testLoader= {len(testLoader)}\")","metadata":{"_uuid":"80613076-7f3d-4c07-b415-ec6ba715a7a6","_cell_guid":"9df5564f-1bad-4c44-9cb6-0711f3831e96","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:52.057811Z","iopub.execute_input":"2023-05-14T19:15:52.059174Z","iopub.status.idle":"2023-05-14T19:15:52.069973Z","shell.execute_reply.started":"2023-05-14T19:15:52.059139Z","shell.execute_reply":"2023-05-14T19:15:52.069015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implement U-Net","metadata":{"_uuid":"cad27c46-8358-490e-b0d8-457daf973d24","_cell_guid":"f05aa31d-b127-4284-9482-f0470646a906","trusted":true}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torchvision.transforms.functional as TF\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n    \nclass AttentionLayer(nn.Module):\n    def __init__(self, num_channels, reduced_channels):\n        super(AttentionLayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Sequential(\n            nn.Linear(num_channels, reduced_channels, bias=False),\n        )\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y)\n        y = self.softmax(y).view(b, -1, 1, 1)\n        return x * y\n    \nclass UNET(nn.Module):\n    def __init__(\n            self, in_channels=3, out_channels=4, features=[64,128,256,512],\n    ):\n        super(UNET, self).__init__()\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.attentions_down = nn.ModuleList()  \n        self.attentions_up = nn.ModuleList()\n        self.attention1=AttentionLayer(features[-1],features[-1])\n        self.attention2=AttentionLayer(features[-1]*2,features[-1]*2)\n\n        # Down part of UNET\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            self.attentions_down.append(AttentionLayer(feature, feature))  \n            in_channels = feature\n\n        # Up part of UNET\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(\n                    feature*2, feature, kernel_size=2, stride=2,\n                )\n            )\n            self.ups.append(DoubleConv(feature*2, feature))\n            self.attentions_up.append(AttentionLayer(feature, feature))  \n\n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0], out_channels, 3,1,1,bias=False)\n\n    def forward(self, x):\n        skip_connections = []\n        \n        #print(\"Down conv part \")\n        for idx, (down, attention) in enumerate(zip(self.downs, self.attentions_down)):\n            x = down(x)\n            x = attention(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n            #print(f\"idx= {idx} shape={x.shape}\")\n        #print(\"Dim after attention: \",x.shape)\n        x = self.attention1(x)\n        x = self.bottleneck(x)\n        x = self.attention2(x)\n        skip_connections = skip_connections[::-1]\n        \n        #print(\"Up conv part \")\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n\n            if x.shape != skip_connection.shape:\n                x = TF.resize(x, size=skip_connection.shape[2:])\n            #print(f\"idx={idx} shape={x.shape}\")\n\n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](concat_skip)\n            x = self.attentions_up[idx // 2](x)  \n        return self.final_conv(x)","metadata":{"_uuid":"ae0366f2-f312-4602-bc14-7cf90bb5c8c6","_cell_guid":"55e20a77-6e9f-45de-a6ce-c2a7ec488a97","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:52.085175Z","iopub.execute_input":"2023-05-14T19:15:52.085825Z","iopub.status.idle":"2023-05-14T19:15:52.120939Z","shell.execute_reply.started":"2023-05-14T19:15:52.085789Z","shell.execute_reply":"2023-05-14T19:15:52.119930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_weights_xavier_normal(layer):\n    if isinstance(layer, (nn.Conv2d, nn.ConvTranspose2d)):\n        init.xavier_normal_(layer.weight)\n        if layer.bias is not None:\n            init.constant_(layer.bias, 0.0)\n    elif isinstance(layer, nn.BatchNorm2d):\n        init.constant_(layer.weight, 1.0)\n        init.constant_(layer.bias, 0.0)","metadata":{"_uuid":"781ef856-8500-4657-9ca2-8b828d3d19e0","_cell_guid":"10ac52b5-d24c-47f7-a284-861efac04f4a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:52.122790Z","iopub.execute_input":"2023-05-14T19:15:52.123539Z","iopub.status.idle":"2023-05-14T19:15:52.137717Z","shell.execute_reply.started":"2023-05-14T19:15:52.123503Z","shell.execute_reply":"2023-05-14T19:15:52.136519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train and Validate","metadata":{"_uuid":"2cfb3949-8947-48e5-a711-4e79aefbfcac","_cell_guid":"19e2d9af-c208-4e24-b90c-7ae139d86339","trusted":true}},{"cell_type":"code","source":"model = UNET(in_channels=3, out_channels=4).to(DEVICE)\n#Apply weight initialization\nmodel.apply(init_weights_xavier_normal)\nloss_func =nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LR)\n\n#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n\nscaler = torch.cuda.amp.GradScaler()\nclip_value=1.6","metadata":{"_uuid":"39274b0e-4fbe-41dd-a5db-53ae81d591a6","_cell_guid":"843dc81e-2a0d-4c79-9017-470417b7ac4e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:52.139431Z","iopub.execute_input":"2023-05-14T19:15:52.140173Z","iopub.status.idle":"2023-05-14T19:15:55.744602Z","shell.execute_reply.started":"2023-05-14T19:15:52.140136Z","shell.execute_reply":"2023-05-14T19:15:55.743560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Choose a part to pass to the U-NET\nimport random\ndef choose(L):\n    x=random.choice(L)\n    L.remove(x)\n    return x","metadata":{"_uuid":"b62af70c-643e-4344-8c3f-85c12e759b99","_cell_guid":"6502ef6a-6d40-459f-b206-12e9ab920aac","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:55.746095Z","iopub.execute_input":"2023-05-14T19:15:55.746697Z","iopub.status.idle":"2023-05-14T19:15:55.752582Z","shell.execute_reply.started":"2023-05-14T19:15:55.746660Z","shell.execute_reply":"2023-05-14T19:15:55.751424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainer(dataloader,model,optimizer,scaler,loss_fn,clip_value):\n    model.train()\n    total_loss=0\n    for images,masks in tqdm(dataloader):\n        \n        #Split into 4 parts to reduce dimensionality\n        images_parts_list,masks_parts_list=split_into_sixteen(images,masks)\n        L=list(range(0,16))\n        while(len(L)!=0):\n            choice=choose(L)\n            images_part=images_parts_list[choice]\n            masks_part=masks_parts_list[choice]\n            \n            images_part=images_part.to(DEVICE)\n            masks_part=masks_part.to(DEVICE)\n            \n            # forward            \n            with torch.cuda.amp.autocast():\n                predictions = model(images_part)\n                #print(\"predictions shape= \",predictions.shape)\n                #print(\"Mask shape= \",masks_part.shape)\n                loss = loss_fn(predictions, masks_part)\n                #print(\"Loss= \",loss)\n        \n            # backward\n            optimizer.zero_grad()\n            scaler.scale(loss).backward()\n            # Clip gradients based on L2 norm\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss+=loss.item()\n        #print(\"batch processed!\")\n    print(f\"Mean Training Loss={total_loss /len(dataloader)}\")","metadata":{"_uuid":"3c95700a-ae89-4081-9b45-ece11ddf6471","_cell_guid":"0ad42570-763b-4747-851b-9cf11719e480","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:55.754308Z","iopub.execute_input":"2023-05-14T19:15:55.754776Z","iopub.status.idle":"2023-05-14T19:15:55.767438Z","shell.execute_reply.started":"2023-05-14T19:15:55.754739Z","shell.execute_reply":"2023-05-14T19:15:55.766243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test run \n#trainer(trainLoader,model,optimizer,scaler,loss_func) Working!","metadata":{"_uuid":"95032303-2f8e-4762-bc89-2dc1d78910ab","_cell_guid":"61024a0a-4c47-4eab-9b6e-886131a6e57e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:55.770372Z","iopub.execute_input":"2023-05-14T19:15:55.771492Z","iopub.status.idle":"2023-05-14T19:15:55.778849Z","shell.execute_reply.started":"2023-05-14T19:15:55.771453Z","shell.execute_reply":"2023-05-14T19:15:55.777776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_one_hot(predicted):\n    # Find the class with the highest probability along the channel dimension\n    max_indices = torch.argmax(predicted, dim=1)\n\n    # Convert the max_indices tensor to one-hot encoding\n    one_hot_predicted = F.one_hot(max_indices, num_classes=predicted.size(1))\n\n    # Rearrange dimensions to match the original tensor (n, c, h, w)\n    one_hot_predicted = one_hot_predicted.permute(0, 3, 1, 2).to(predicted.dtype)\n\n    return one_hot_predicted","metadata":{"_uuid":"fadcc418-f88b-4705-a91b-f70ecdca48b6","_cell_guid":"24c4e30c-fa2a-4aa2-97c9-a098c9a97da4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:55.780204Z","iopub.execute_input":"2023-05-14T19:15:55.780753Z","iopub.status.idle":"2023-05-14T19:15:55.790006Z","shell.execute_reply.started":"2023-05-14T19:15:55.780713Z","shell.execute_reply":"2023-05-14T19:15:55.789078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_score(predicted, target, eps=1e-7):\n    assert predicted.size() == target.size(), \"Input tensors must have the same shape\"\n    \n    # Initialize an empty tensor to store the Dice scores for each class\n    dice_scores = torch.zeros(predicted.size(1), device=predicted.device, dtype=predicted.dtype)\n\n    # Iterate over each class and calculate the Dice score\n    for class_idx in range(predicted.size(1)):\n        pred_class = predicted[:, class_idx].contiguous().view(predicted.size(0), -1)\n        target_class = target[:, class_idx].contiguous().view(target.size(0), -1)\n\n        intersection = (pred_class * target_class).sum(dim=1)\n        volumes = pred_class.sum(dim=1) + target_class.sum(dim=1)\n\n        dice_scores[class_idx] = ((2 * intersection + eps) / (volumes + eps)).mean()\n\n    return dice_scores","metadata":{"_uuid":"873f5771-c1c9-4312-8cee-8a3973a9791a","_cell_guid":"ceedabbb-ec68-4657-8bee-3ca315f794e0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:55.791746Z","iopub.execute_input":"2023-05-14T19:15:55.792109Z","iopub.status.idle":"2023-05-14T19:15:55.805657Z","shell.execute_reply.started":"2023-05-14T19:15:55.792074Z","shell.execute_reply":"2023-05-14T19:15:55.804635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_performance(loader, model, device=\"cuda\"):\n    num_correct = 0\n    num_pixels = 1920*1080*BATCH_SIZE*len(loader)\n    total_ds = 0\n    rows=4\n    cols=4\n    model.eval()\n    with torch.no_grad():\n        for images, masks in tqdm(loader):\n            #Split each image and masks in batch into 16 parts\n            images_parts_list,masks_parts_list=split_into_sixteen(images,masks)\n            \n            masked_preds_list=[]\n            dice_class=0\n            #print(\"Checking accuracy\")\n            for i in range(16):\n                images_part=images_parts_list[i]\n                masks_part=masks_parts_list[i]\n                \n                images_part=images_part.to(DEVICE)\n                masks_part=masks_part.to(DEVICE)\n                #print(f\"image part shape={images_part.shape}\")\n                preds = model(images_part)\n                \n                preds = F.softmax(preds, dim=1)\n                max_indices = torch.argmax(preds, dim=1)\n                masked_preds=convert_to_one_hot(preds)\n                #print(f\"image part(masked pred) shape={masked_preds.shape}\")\n                masked_preds_list.append(masked_preds)\n                \n                arg_masks =torch.argmax(masks_part,dim=1)\n                num_correct += (max_indices == arg_masks).sum()\n            \n            mask_rows=[]\n            for i in range(rows):\n                mask_row=torch.cat(masked_preds_list[i*cols: (i+1)*cols],dim=-1)\n                mask_rows.append(mask_row)\n   \n            #upper_part_masked_pred=torch.cat((masked_preds_list[0],masked_preds_list[1]),dim=3)\n            #print(\"Upper part shape= \",upper_part_masked_pred.shape)\n            #lower_part_masked_pred=torch.cat((masked_preds_list[2],masked_preds_list[3]),dim=3)\n            #print(\"Lower part shape= \",lower_part_masked_pred.shape)\n            \n            #whole_masked_pred=torch.cat((upper_part_masked_pred,lower_part_masked_pred),dim=2)#Along height dim\n            whole_masked_pred=torch.cat(mask_rows,dim=-2)\n            #print(\"Whole predicted mask shape= \",whole_masked_pred.shape)\n            #upper_part_masked_true=torch.cat((masks_parts_list[0],masks_parts_list[1]),dim=3)\n            #lower_part_masked_true=torch.cat((masks_parts_list[2],masks_parts_list[3]),dim=3)\n            whole_masked_true=masks.to(DEVICE)\n            #print(\"Whole true mask shape= \",whole_masked_true.shape)\n            ds=sum(dice_score(whole_masked_pred,whole_masked_true))/whole_masked_true.shape[1]\n            #print(\"ds= \",ds)\n            total_ds+=ds\n            \n                \n    acc=num_correct/num_pixels\n    print(\n        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n    )\n    mds=total_ds/len(loader)\n    print(f\"Mean Dice score: {mds}\")\n    model.train()\n    return mds,acc","metadata":{"_uuid":"2391c909-56b6-4f22-b868-04bd2fdd79d6","_cell_guid":"0b2d92df-4132-43ef-9f76-e95d8816b82b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:55.807383Z","iopub.execute_input":"2023-05-14T19:15:55.807881Z","iopub.status.idle":"2023-05-14T19:15:55.823612Z","shell.execute_reply.started":"2023-05-14T19:15:55.807844Z","shell.execute_reply":"2023-05-14T19:15:55.822657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"9dbc7450-6255-4f40-a974-0ccc8a56df09","_cell_guid":"55e8df9d-f308-46c4-a943-c42ca33ea993","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving and Loading","metadata":{"_uuid":"c3043743-051a-495d-b08c-22717143bfc1","_cell_guid":"372df94e-400f-4cd0-8138-818684062029","trusted":true}},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"checkpoint.pth\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\ndef load_checkpoint(checkpoint, model):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"],strict=False)\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    start_epoch=checkpoint['epoch']\n    print(f\"start epoch= \",start_epoch)\n    return start_epoch","metadata":{"_uuid":"c78bf24c-0aea-46e3-b19d-40e2bf32bd31","_cell_guid":"810422ca-50ca-4e74-8a52-5bba080b1970","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:55.825009Z","iopub.execute_input":"2023-05-14T19:15:55.825470Z","iopub.status.idle":"2023-05-14T19:15:55.839048Z","shell.execute_reply.started":"2023-05-14T19:15:55.825431Z","shell.execute_reply":"2023-05-14T19:15:55.837886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"_uuid":"c5fada3f-3154-435b-86c6-ffc4cef83cf0","_cell_guid":"ddafbb78-8cf3-4748-bb8f-ecd287ebe31e","trusted":true}},{"cell_type":"code","source":"%cp /kaggle/input/unetsmallestcheckpoint/checkpoint.pth .\n%ls","metadata":{"_uuid":"a503ab95-bba8-456f-8452-6299eaa71455","_cell_guid":"60d69878-0011-4ff8-a1be-e2fa3e895fa6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:15:55.842044Z","iopub.execute_input":"2023-05-14T19:15:55.842723Z","iopub.status.idle":"2023-05-14T19:16:02.099657Z","shell.execute_reply.started":"2023-05-14T19:15:55.842694Z","shell.execute_reply":"2023-05-14T19:16:02.097871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mds=0 #\nif os.path.exists(\"checkpoint.pth\"):\n    print(\"Exists\")\n    LOAD_MODEL=True\n\n\nif LOAD_MODEL:\n    start_epoch=load_checkpoint(torch.load(\"checkpoint.pth\"), model)\n    print(\"Model loaded\")\nelse:\n    start_epoch=0\n#Accuracy of a random model -Testing\nmds,acc=check_performance(testLoader, model, device=DEVICE)\n\n#Loop\nfor epoch in range(start_epoch,EPOCHS):\n    print(f\"EPOCH #{epoch}\")\n    print(\"*\"*100)\n    #call train\n    trainer(trainValLoader,model,optimizer,scaler,loss_func,clip_value)\n    #Save model\n    checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\":optimizer.state_dict(),\n            \"epoch\":epoch+1,\n        }\n    save_checkpoint(checkpoint)\n    \n    #Check accuracy\n    re_mds,re_acc=check_performance(testLoader,model,device=DEVICE)\n    #Save best model \n    if (re_mds>mds):\n        save_checkpoint(checkpoint,filename=\"best_checkpoint.pth\")\n        print(f\"Best checkpoint found at epoch={epoch}\")\n        mds=re_mds\n    # Update the learning rate using ROPLR\n    val_loss=1-re_mds\n    #scheduler.step(val_loss)","metadata":{"_uuid":"51d588bd-f18a-4692-98e8-de84825de84d","_cell_guid":"0f5bf4ca-b5c4-4736-b1a0-90a348713311","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:16:02.107157Z","iopub.execute_input":"2023-05-14T19:16:02.108676Z","iopub.status.idle":"2023-05-14T19:28:26.785150Z","shell.execute_reply.started":"2023-05-14T19:16:02.108611Z","shell.execute_reply":"2023-05-14T19:28:26.783472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pretrain model - test","metadata":{"_uuid":"5e40ae7e-02bd-47b0-ae33-04882e1fbffc","_cell_guid":"9b620eb4-abd4-4c9e-8cd3-76c7cd1758ea","trusted":true}},{"cell_type":"code","source":"\"\"\"from torch import nn\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch.losses import DiceLoss\n\nENCODER=\"efficientnet-b2\"\nWEIGHT=\"imagenet\"\n\"\"\"","metadata":{"_uuid":"78d3ef7f-77ad-4aee-9aef-6030406dcc31","_cell_guid":"1cd17439-f43d-4d7e-85d1-3dd7c3b1ced7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:28:26.786519Z","iopub.status.idle":"2023-05-14T19:28:26.787326Z","shell.execute_reply.started":"2023-05-14T19:28:26.787028Z","shell.execute_reply":"2023-05-14T19:28:26.787054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"class PretrainedModel(nn.Module):\n    def __init__(self):\n        super(PretrainedModel,self).__init__()\n        self.architecture=smp.Unet(encoder_name=ENCODER,\n                                  encoder_weights=WEIGHT,\n                                    in_channels=3,\n                                   classes=4,\n                                  activation='softmax')\n    def forward(self,images,masks=None):\n        soft_prob=self.architecture(images)\n        if (masks):\n            loss=DiceLoss('multiclass')(soft_prob,masks)\n            return soft_prob,loss\n        return soft_prob\n\"\"\"","metadata":{"_uuid":"4f880400-98e0-4518-bd1d-dd4de600642e","_cell_guid":"8e866107-0233-46a5-845d-c0cbf5faf1b8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-14T19:28:26.789239Z","iopub.status.idle":"2023-05-14T19:28:26.789759Z","shell.execute_reply.started":"2023-05-14T19:28:26.789502Z","shell.execute_reply":"2023-05-14T19:28:26.789526Z"},"trusted":true},"execution_count":null,"outputs":[]}]}